{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Classifier: Advanced Machine Learning\n",
    "# The Super Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Add more packages as required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import re\n",
    "from itertools import combinations\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/19e9/c732082706f39d2ba12845851309714db135.pdf) but the stacked ensemble idea has been around for a long time. \n",
    "\n",
    "Figure 1 shows a flow diagram of the Super Learner process (this is from (van der Laan et al, 2007) and the process is also described in the COMP47590 lecture \"[COMP47590 2017-2018 L04 Supervised Learning Ensembles 3](https://www.dropbox.com/s/1ksx94nxtuyn4l8/COMP47590%202017-2018%20L04%20Supervised%20Learning%20Ensembles%203.pdf?raw=1)\"). The base classifiers are trained and their outputs are combined along with the training dataset labels into a training set for the stack layer classifier. To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process (described as V-fold in Figure 1). To further add variety to the base estimators a bootstrapping selection (as is used in the bagging ensemble approach).\n",
    " \n",
    "![Super Learner Process Flow](SuperLearnerProcessFlow.png \"Logo Title Text 1\")\n",
    "Figure 1: A flow diagram for the Super Learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of available base classifier for our Superlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2018-03-09 14:52:20.212982'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start Time: \")\n",
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_base_lr = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "clf_base_rf = RandomForestClassifier(n_estimators=51, max_depth = 4, min_samples_split = 50)\n",
    "clf_base_nb = GaussianNB()\n",
    "clf_base_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_base_dt = DecisionTreeClassifier(max_depth = 4, min_samples_split = 50 )\n",
    "clf_base_svc = SVC(kernel = 'linear', probability=True)\n",
    "clf_base_sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "clf_base_ada = AdaBoostClassifier(n_estimators=51)\n",
    "clf_base_gb = GradientBoostingClassifier(n_estimators=50, learning_rate=0.2)\n",
    "\n",
    "clf_base_default = [clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc]\n",
    "clf_base_all = [clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc, clf_base_sgd, clf_base_ada, clf_base_gb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of available stack classifier for our Superlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_stack_lr = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "clf_stack_rf = RandomForestClassifier(n_estimators=51, max_depth = 4, min_samples_split = 50)\n",
    "clf_stack_nb = GaussianNB()\n",
    "clf_stack_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_stack_dt = DecisionTreeClassifier(max_depth = 4, min_samples_split = 50 )\n",
    "clf_stack_svc = SVC(kernel = 'linear', probability=True)\n",
    "clf_stack_sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "clf_stack_ada = AdaBoostClassifier(n_estimators=51)\n",
    "clf_stack_gb = GradientBoostingClassifier(n_estimators=50, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, clfs=clf_base_default, stacked_clf=clf_stack_dt, training='label', useEntireData='False', randomN=None):\n",
    "    \n",
    "        # If randomN parameter is passed then overwrite the clfs parameter and select N random classifer from our overall list\n",
    "        if randomN:\n",
    "            clfs=random.sample(clf_base_all, k=randomN)\n",
    "            print(\"Random\", randomN, \"classifiers chosen\")\n",
    "            print(clfs)\n",
    "        \n",
    "        # Set parameter values\n",
    "        self.clfs = clfs\n",
    "        self.stacked_clf = stacked_clf\n",
    "        self.training = training\n",
    "        self.kFolds = 4\n",
    "        self.useEntireData = useEntireData\n",
    "        \n",
    "        \n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "            \n",
    "        # Check if X & y have similar dimensions\n",
    "        X, y = check_X_y(X, y)\n",
    "            \n",
    "        # Set number of folds\n",
    "        kf = KFold(n_splits=self.kFolds)\n",
    "            \n",
    "        fit_pred_one = pd.DataFrame()     \n",
    "        self.y_kfold_data = []\n",
    "        self.stackData = pd.DataFrame()\n",
    "        \n",
    "        # Create input data for stacked model using 4-Fold \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "            y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "            self.y_kfold_data.extend(y_test)\n",
    "            fit_pred = pd.DataFrame()\n",
    "            for clf in self.clfs:\n",
    "                \n",
    "                # Bagging of classifier\n",
    "                clf = BaggingClassifier(clf)\n",
    "                \n",
    "                clf.fit(X_train,y_train)\n",
    "                \n",
    "                # Check if labels or probability is required as the output from the base classifiers\n",
    "                if self.training == 'label':\n",
    "                    fit_pred_one = clf.predict(X_test)\n",
    "                else:\n",
    "                    fit_pred_one = clf.predict_proba(X_test)\n",
    "                fit_pred_one = pd.DataFrame(fit_pred_one)\n",
    "                \n",
    "                # Append data columns wise and rows wise \n",
    "                fit_pred = pd.concat([fit_pred, fit_pred_one], axis = 1)\n",
    "            self.stackData = pd.concat([self.stackData, fit_pred], axis = 0)\n",
    "         \n",
    "        # Fit all(100%) data in base learners once we have created input data for our stacked layer\n",
    "        for clf in self.clfs:\n",
    "            clf.fit(X,y)    \n",
    "        \n",
    "        # Add raw data if needed\n",
    "        if self.useEntireData == 'True':\n",
    "            self.stackData.reset_index(inplace=True, drop=True)\n",
    "            X_temp = pd.DataFrame(X).reset_index(drop=True)\n",
    "            self.stackData = pd.concat([self.stackData, X_temp], axis = 1)\n",
    "                \n",
    "        # Fit data into stacked classifier\n",
    "        self.stacked_clf.fit(self.stackData, self.y_kfold_data)\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stackData'])\n",
    "\n",
    "        pred_one = pd.DataFrame()\n",
    "        pred = pd.DataFrame()\n",
    "        \n",
    "        # Create dataset similar to what was fitted to the stacked layer\n",
    "        for clf in self.clfs:\n",
    "            if self.training == 'label':\n",
    "                pred_one = clf.predict(X)\n",
    "            else:\n",
    "                pred_one = clf.predict_proba(X)\n",
    "            pred_one = pd.DataFrame(pred_one)\n",
    "            pred = pd.concat([pred, pred_one], axis = 1)\n",
    "            \n",
    "        if self.useEntireData == 'True':\n",
    "            pred.reset_index(inplace=True, drop=True)\n",
    "            X_temp = pd.DataFrame(X).reset_index(drop=True)\n",
    "            pred = pd.concat([pred, X_temp], axis = 1)\n",
    "        \n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        pred = check_array(pred)\n",
    "        \n",
    "        # Predict labels using stacked classifier\n",
    "        stacked_pred = self.stacked_clf.predict(pred)\n",
    "        \n",
    "        # Return predictions\n",
    "        return stacked_pred\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stackData'])\n",
    "\n",
    "        pred_one = pd.DataFrame()\n",
    "        pred = pd.DataFrame()\n",
    "        \n",
    "         # Create dataset similar to what was fitted to the stacked layer\n",
    "        for clf in self.clfs:\n",
    "            if self.training == 'label':\n",
    "                pred_one = clf.predict(X)\n",
    "            else:\n",
    "                pred_one = clf.predict_proba(X)\n",
    "            pred_one = pd.DataFrame(pred_one)\n",
    "            pred = pd.concat([pred, pred_one], axis = 1)\n",
    "            \n",
    "        if self.useEntireData == 'True':\n",
    "            pred.reset_index(inplace=True, drop=True)\n",
    "            X_temp = pd.DataFrame(X).reset_index(drop=True)\n",
    "            pred = pd.concat([pred, X_temp], axis = 1)\n",
    "            \n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        pred = check_array(pred)\n",
    "        \n",
    "        # Predict probabilities using stacked classifier\n",
    "        stacked_proba_pred = self.stacked_clf.predict_proba(pred)\n",
    "        \n",
    "        # Return predicted probabilities\n",
    "        return stacked_proba_pred\n",
    "    \n",
    "    def base_model_eval(self):\n",
    "        \n",
    "        eval_data = self.stackData\n",
    "        y_original = self.y_kfold_data\n",
    "        base_model_accuracy_comparison = dict()\n",
    "        used_model_list = []\n",
    "        \n",
    "        # Calculate accuracy of every base classifier\n",
    "        count = 0\n",
    "        for column in eval_data:\n",
    "            accuracy = metrics.accuracy_score(y_original, eval_data.iloc[:,count])\n",
    "            clf_string = str(self.clfs[count])\n",
    "            model = re.split(r'\\(', clf_string)[0]\n",
    "            base_model_accuracy_comparison[model] = accuracy\n",
    "            used_model_list.append(model)\n",
    "            count += 1 \n",
    "            \n",
    "        # Calculate the diversity between base classifiers by finding the percent of overlap between them.\n",
    "        # More the overlap, less diversity will be present and vice versa\n",
    "        eval_data.columns = used_model_list\n",
    "        val = eval_data.values\n",
    "        model_overlap_dict = {(i, j): np.mean(val[:, i] == val[:, j]) for i, j in combinations(range(val.shape[1]), 2)}\n",
    "        result, c, vals = np.zeros((val.shape[1], val.shape[1])), \\\n",
    "                   list(map(list, zip(*model_overlap_dict.keys()))), list(model_overlap_dict.values())\n",
    "\n",
    "        result[c[0], c[1]] = vals\n",
    "\n",
    "        model_overlap = pd.DataFrame(result, columns=used_model_list, index=used_model_list)\n",
    "           \n",
    "        # Return overlap between models along with their accuracy\n",
    "        return model_overlap, base_model_accuracy_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the SuperLearnClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93333333, 0.93333333, 1.        , 1.        , 0.86666667,\n",
       "       1.        , 0.93333333, 0.66666667, 1.        , 1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "clf = SuperLearnerClassifier()\n",
    "iris = load_iris()\n",
    "clf.fit(iris.data, iris.target)\n",
    "cross_val_score(clf, iris.data, iris.target, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the number of folds for all grid searches (should be 5 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52634</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19416</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12820</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35250</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7782</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>122</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "52634      1       0       0       0       0       0       0       0      12   \n",
       "19416      9       0       0       0       0       0       0       0       0   \n",
       "12820      5       0       0       0       0       0       0       0       0   \n",
       "35250      7       0       0       0       0       0       0       0       0   \n",
       "7782       6       0       0       0       0       0       0       0       2   \n",
       "\n",
       "       pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "52634      15    ...           20        16        15        16         0   \n",
       "19416       0    ...            0         0         0         0         0   \n",
       "12820       0    ...            0         0         0         0         0   \n",
       "35250       0    ...            0         0         0         0         0   \n",
       "7782        2    ...            0         0         0         0       115   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "52634         0         0         0         0         0  \n",
       "19416         0         0         0         0         0  \n",
       "12820         0         0         0         0         0  \n",
       "35250         0         0         0         0         0  \n",
       "7782        122        34         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data pre-processing and manipulation as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here\n",
    "X = dataset[dataset.columns[1:]]\n",
    "Y = np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train and Evaluate a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Super Learner Classifier using the prepared dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Call SuperLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call superlearner for predefined 6 base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False), RandomForestClassifier(boots...r',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)],\n",
       "            randomN=None,\n",
       "            stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "            training='label', useEntireData='False')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Below is the list of default classifiers which were included at the beginning of the code\n",
    "# clf_base_default = [clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc]\n",
    "# Pass the clf_base_default value to the 'clf' parameter\n",
    "\n",
    "my_model_q1 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')\n",
    "my_model_q1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7711111111111111\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.79      0.77       104\n",
      "          1       0.88      0.94      0.91        89\n",
      "          2       0.60      0.69      0.64        97\n",
      "          3       0.77      0.76      0.77        84\n",
      "          4       0.68      0.67      0.67        90\n",
      "          5       0.85      0.88      0.87       107\n",
      "          6       0.41      0.30      0.35        64\n",
      "          7       0.79      0.83      0.81        72\n",
      "          8       0.88      0.75      0.81        87\n",
      "          9       0.93      0.93      0.93       106\n",
      "\n",
      "avg / total       0.77      0.77      0.77       900\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>109</td>\n",
       "      <td>95</td>\n",
       "      <td>112</td>\n",
       "      <td>83</td>\n",
       "      <td>88</td>\n",
       "      <td>110</td>\n",
       "      <td>46</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>107</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1    2   3   4    5   6   7   8    9  All\n",
       "True                                                      \n",
       "0           82   3    8   2   0    0   7   0   2    0  104\n",
       "1            2  84    0   3   0    0   0   0   0    0   89\n",
       "2            1   0   67   1  17    1   8   0   2    0   97\n",
       "3            5   7    2  64   1    1   4   0   0    0   84\n",
       "4            0   0   12  10  60    1   7   0   0    0   90\n",
       "5            0   0    1   0   1   94   0   8   1    2  107\n",
       "6           12   1   15   1   8    8  19   0   0    0   64\n",
       "7            0   0    0   0   0    4   0  60   2    6   72\n",
       "8            7   0    6   2   1    1   1   4  65    0   87\n",
       "9            0   0    1   0   0    0   0   4   2   99  106\n",
       "All        109  95  112  83  88  110  46  76  74  107  900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model_q1.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "#print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Cross Validation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79787234, 0.77659574, 0.83870968, 0.75268817, 0.82222222,\n",
       "       0.76666667, 0.76404494, 0.76744186, 0.76744186, 0.76470588])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code here\n",
    "my_model_q2 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_rf, training='label')\n",
    "\n",
    "#my_model.fit(X_train, y_train)\n",
    "cross_validation.cross_val_score(my_model_q2, X_test, y_test, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy varies from 75% to 83% during the cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Label vs Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the labels as the training dataset for the stacked layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  0  0  0  0  0\n",
       "0  4  4  4  2  2  4\n",
       "1  3  0  0  0  0  0\n",
       "2  4  4  4  4  4  4\n",
       "3  0  0  0  0  0  0\n",
       "4  4  3  3  4  3  4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set \"training='label'\" to train on the labels\n",
    "my_model_q3_label = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_nb, training='label')\n",
    "my_model_q3_label.fit(X_train, y_train)\n",
    "    \n",
    "# Display head of input data of stack layer : labels\n",
    "my_model_q3_label.stackData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the probabilities as the input for the staked classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.527835e-02</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.137461</td>\n",
       "      <td>3.186668e-02</td>\n",
       "      <td>0.387149</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>0.395731</td>\n",
       "      <td>4.662876e-05</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058083</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>0.224443</td>\n",
       "      <td>0.049820</td>\n",
       "      <td>0.398173</td>\n",
       "      <td>0.018329</td>\n",
       "      <td>0.200064</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>0.008545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.239749e-01</td>\n",
       "      <td>0.215332</td>\n",
       "      <td>0.105032</td>\n",
       "      <td>2.857483e-01</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.033912</td>\n",
       "      <td>6.754633e-02</td>\n",
       "      <td>0.019794</td>\n",
       "      <td>0.044825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289236</td>\n",
       "      <td>0.149530</td>\n",
       "      <td>0.046699</td>\n",
       "      <td>0.136599</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.098244</td>\n",
       "      <td>0.161853</td>\n",
       "      <td>0.052936</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.016113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.155764e-07</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.109816</td>\n",
       "      <td>4.462309e-08</td>\n",
       "      <td>0.825653</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.064407</td>\n",
       "      <td>1.252925e-09</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.179108</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.554086</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.245844</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.398384e-01</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.018504</td>\n",
       "      <td>5.506390e-04</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>4.840406e-06</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837494</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.120149</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.002027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.548935e-03</td>\n",
       "      <td>0.176716</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>7.818318e-03</td>\n",
       "      <td>0.798809</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>2.448623e-04</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050779</td>\n",
       "      <td>0.234571</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>0.081191</td>\n",
       "      <td>0.451092</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.094533</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.026643</td>\n",
       "      <td>0.009069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2             3         4         5  \\\n",
       "0  1.527835e-02  0.002283  0.137461  3.186668e-02  0.387149  0.011911   \n",
       "1  2.239749e-01  0.215332  0.105032  2.857483e-01  0.001673  0.002162   \n",
       "2  2.155764e-07  0.000073  0.109816  4.462309e-08  0.825653  0.000004   \n",
       "3  9.398384e-01  0.000146  0.018504  5.506390e-04  0.000424  0.000014   \n",
       "4  9.548935e-03  0.176716  0.000038  7.818318e-03  0.798809  0.000014   \n",
       "\n",
       "          6             7         8         9    ...            0         1  \\\n",
       "0  0.395731  4.662876e-05  0.016255  0.002019    ...     0.058083  0.014135   \n",
       "1  0.033912  6.754633e-02  0.019794  0.044825    ...     0.289236  0.149530   \n",
       "2  0.064407  1.252925e-09  0.000044  0.000004    ...     0.003099  0.006804   \n",
       "3  0.038226  4.840406e-06  0.002121  0.000171    ...     0.837494  0.003725   \n",
       "4  0.005220  2.448623e-04  0.000794  0.000798    ...     0.050779  0.234571   \n",
       "\n",
       "          2         3         4         5         6         7         8  \\\n",
       "0  0.224443  0.049820  0.398173  0.018329  0.200064  0.007199  0.021210   \n",
       "1  0.046699  0.136599  0.024198  0.098244  0.161853  0.052936  0.024594   \n",
       "2  0.179108  0.001122  0.554086  0.001335  0.245844  0.001801  0.005598   \n",
       "3  0.014063  0.007505  0.005835  0.001165  0.120149  0.001284  0.006753   \n",
       "4  0.030952  0.081191  0.451092  0.009392  0.094533  0.011777  0.026643   \n",
       "\n",
       "          9  \n",
       "0  0.008545  \n",
       "1  0.016113  \n",
       "2  0.001204  \n",
       "3  0.002027  \n",
       "4  0.009069  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set \"training='probaility'\" to train on the labels\n",
    "my_model_q3_proba = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='probability')\n",
    "my_model_q3_proba.fit(X_train, y_train)\n",
    "\n",
    "# Display head of input data of stack layer : probability\n",
    "my_model_q3_proba.stackData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Different Models for stack layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using of different classifiers in the stacked layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8011111111111111\n"
     ]
    }
   ],
   "source": [
    "# Using Decision Tree by changing value for 'stacked_clf' parameter\n",
    "my_model_q4 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_dt, training='label')\n",
    "my_model_q4.fit(X_train, y_train)\n",
    "y_pred = my_model_q4.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5844444444444444\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression by changing value for 'stacked_clf' parameter\n",
    "my_model_q4 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_lr, training='label')\n",
    "my_model_q4.fit(X_train, y_train)\n",
    "y_pred = my_model_q4.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Accuracy can vary drastically depending on what model we use at the stacked layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Comparing the Performance of Different Stack Layer Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DT_Label': 0.7966666666666666,\n",
       " 'DT_Proba': 0.6233333333333333,\n",
       " 'LR_Label': 0.5411111111111111,\n",
       " 'LR_Proba': 0.8288888888888889}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keeping the 'clf' constant change the value of 'stacked_clf' and 'training' parameter\n",
    "model_test_accuracy_comparisons = dict()\n",
    "\n",
    "my_model_q5_1 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_dt, training='label')\n",
    "my_model_q5_1.fit(X_train, y_train)\n",
    "y_pred = my_model_q5_1.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "model_test_accuracy_comparisons[\"DT_Label\"] = accuracy\n",
    "\n",
    "my_model_q5_2 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_dt, training='probability')\n",
    "my_model_q5_2.fit(X_train, y_train)\n",
    "y_pred = my_model_q5_2.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "model_test_accuracy_comparisons[\"DT_Proba\"] = accuracy\n",
    "\n",
    "my_model_q5_3 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_lr, training='label')\n",
    "my_model_q5_3.fit(X_train, y_train)\n",
    "y_pred = my_model_q5_3.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "model_test_accuracy_comparisons[\"LR_Label\"] = accuracy\n",
    "\n",
    "my_model_q5_4 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_lr, training='probability')\n",
    "my_model_q5_4.fit(X_train, y_train)\n",
    "y_pred = my_model_q5_4.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "model_test_accuracy_comparisons[\"LR_Proba\"] = accuracy\n",
    "\n",
    "display(model_test_accuracy_comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD8CAYAAAChHgmuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnWzBj2tgVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12636eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the graph for performance comparison\n",
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')\n",
    "_ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Pass the base classifiers as a specific list or select random N number of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass as a specific list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5288888888888889\n"
     ]
    }
   ],
   "source": [
    "# Pass a list of classifier to the 'clfs' parameter\n",
    "my_model_q6 = SuperLearnerClassifier(clfs=[clf_base_knn,clf_base_svc, clf_base_nb], stacked_clf=clf_stack_lr, training='label')\n",
    "my_model_q6.fit(X_train, y_train)\n",
    "y_pred = my_model_q6.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select random classifiers as per the number passed via the parameter.\n",
    "\n",
    "When 'randomN' parameter is passed it will override the 'clfs' parameter and will select random N number of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 4 classifiers chosen\n",
      "Accuracy: 0.5366666666666666\n"
     ]
    }
   ],
   "source": [
    "# Pass the parameter 'randomN' with the number of random base classifier required\n",
    "my_model_q6_b = SuperLearnerClassifier(clfs=[clf_base_knn,clf_base_svc, clf_base_nb], stacked_clf=clf_stack_lr, training='label', randomN=4)\n",
    "my_model_q6_b.fit(X_train, y_train)\n",
    "y_pred = my_model_q6_b.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters (Task 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a grid search experiment to detemrine the optimal architecture and hyper-parameter values for the SuperLearnClasssifier for the MNIST Fashion classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried experimenting with the different parameters but it's getting computationally expensive.\n",
    "So for now I have tried to keep things minimum to show the functionality of the GridSearch with SuperLearner. Following number of parameters are used:\n",
    "\n",
    "clfs: 1<br>\n",
    "stacked_clf: 2<br>\n",
    "training: 2<br>\n",
    "cv: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=label \n",
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=label, total=  53.1s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=label \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=label, total=  51.6s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=probability \n",
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=probability, total=  51.4s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=probability \n",
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), training=probability, total=  52.0s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=label \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=label, total=  52.6s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=label \n",
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=label, total=  50.5s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=probability \n",
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=probability, total=  52.1s\n",
      "[CV] clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=probability \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clfs=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], stacked_clf=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), training=probability, total=  51.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'clfs': [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=51, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False), GaussianNB(priors=None), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)], 'stacked_clf': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), 'training': 'probability'}\n",
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'clfs': [[clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc]],\n",
    "  'stacked_clf': [clf_stack_knn, clf_stack_dt ],\n",
    "  'training': ['label', 'probability']\n",
    " }\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = GridSearchCV(SuperLearnerClassifier(), param_grid, cv=2, verbose = 2, n_jobs=1)\n",
    "my_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model selected by the grid search on a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy for the best parameters selected by GridSearch\n",
    "my_model_q7_eval = SuperLearnerClassifier(clfs=[clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc], stacked_clf=clf_stack_knn, training='probability')\n",
    "my_model_q7_eval.fit(X_train, y_train)\n",
    "y_pred = my_model_q7_eval.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred)\n",
    "print(\"Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Initially during our cross validation we were getting accuracy around 77%. After using different hyper parameters with GridSearch, our accuracy increased to 79.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer (Task 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer.\n",
    "\n",
    "Additional parameter required for that would \"useEntireData=True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7983333333333333\n"
     ]
    }
   ],
   "source": [
    "# Pass the parameter \"useEntireData=True\"\n",
    "my_model_q8 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label', useEntireData='True')\n",
    "my_model_q8.fit(X_train, y_train)\n",
    "y_pred = my_model_q8.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred)\n",
    "print(\"Accuracy: \" +  str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the entire data we can observe that the Accuracy jumped by around 2.7%<br>\n",
    "Without original features: 77.11%<br>\n",
    "With Original features: 79.83%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model (Task 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here\n",
    "my_model_q9 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')\n",
    "my_model_q9.fit(X_train, y_train)\n",
    "\n",
    "# Get the accuracy of each model and the overlap between the model\n",
    "base_model_relation, base_accuracy_comparison = my_model_q9.base_model_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DecisionTreeClassifier': 0.698,\n",
       " 'GaussianNB': 0.5686666666666667,\n",
       " 'KNeighborsClassifier': 0.7573333333333333,\n",
       " 'LogisticRegression': 0.8006666666666666,\n",
       " 'RandomForestClassifier': 0.738,\n",
       " 'SVC': 0.802}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeIAAAD8CAYAAACrQqf8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvL4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a66cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displays the accuracy of all the base classifiers\n",
    "display(base_accuracy_comparison)\n",
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(base_accuracy_comparison)), list(base_accuracy_comparison.values()), align='center')\n",
    "_ = plt.yticks(range(len(base_accuracy_comparison)), list(base_accuracy_comparison.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check diversity between base learners predictions\n",
    "Calculate the overlap between the label output of the classifier. It finds the relation between output by comparing how may value are predicted same by two classifier divided by total number of predictions made.\n",
    "\n",
    "Overlap_ratio = Same Predictions / Total Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAFlCAYAAABbWrnGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125375c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Folliwng heatmap will show the percentage overlap between results of different base models.\n",
    "# There's a scale along with the heatmap which shows the level of overlap. \n",
    "# Its actually a tradeoff between each model accuracy and the overlap across the two. Each model should have high accuracy independently while they shouldn't have too much of an overlap to keep up with the diversity. \n",
    "# The color scale could vary and it could be a good idea to have a diversity in the upper green or yellow zone.\n",
    "\n",
    "cmap = colors.ListedColormap(['grey','green','yellow','red'])\n",
    "bounds=[0, 0.3, 0.7, 0.85, 1]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "plt.pcolor(base_model_relation, cmap=cmap, norm=norm)\n",
    "plt.yticks(np.arange(0.5, len(base_model_relation.index), 1), base_model_relation.index)\n",
    "plt.xticks(np.arange(0.5, len(base_model_relation.columns), 1), base_model_relation.columns, rotation=70)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red block (85% - 100%): High overlap between SVC and LR <br>\n",
    "Yellow block (70% - 85%): Acceptable overlap. They have decent amount of diversity<br>\n",
    "Green block (30% - 70%): Low overlap between the models. High diversity present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>SVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.639333</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.799333</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627333</td>\n",
       "      <td>0.607333</td>\n",
       "      <td>0.632667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        LogisticRegression  RandomForestClassifier  \\\n",
       "LogisticRegression                     0.0                   0.808   \n",
       "RandomForestClassifier                 0.0                   0.000   \n",
       "GaussianNB                             0.0                   0.000   \n",
       "KNeighborsClassifier                   0.0                   0.000   \n",
       "DecisionTreeClassifier                 0.0                   0.000   \n",
       "SVC                                    0.0                   0.000   \n",
       "\n",
       "                        GaussianNB  KNeighborsClassifier  \\\n",
       "LogisticRegression        0.639333              0.806000   \n",
       "RandomForestClassifier    0.692000              0.799333   \n",
       "GaussianNB                0.000000              0.627333   \n",
       "KNeighborsClassifier      0.000000              0.000000   \n",
       "DecisionTreeClassifier    0.000000              0.000000   \n",
       "SVC                       0.000000              0.000000   \n",
       "\n",
       "                        DecisionTreeClassifier       SVC  \n",
       "LogisticRegression                    0.714000  0.906667  \n",
       "RandomForestClassifier                0.826000  0.808000  \n",
       "GaussianNB                            0.607333  0.632667  \n",
       "KNeighborsClassifier                  0.699333  0.826667  \n",
       "DecisionTreeClassifier                0.000000  0.715333  \n",
       "SVC                                   0.000000  0.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overlap score ratio between differnt base models.\n",
    "base_model_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Time (If only it will ever end): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2018-03-09 09:44:02.842640'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"End Time (If only it will ever end): \")\n",
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
